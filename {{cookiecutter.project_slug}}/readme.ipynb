{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c250529d-3381-4cf1-b9e1-da590fa12691",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"README\"\n",
    "author: Chris Keller (ckeller@ap.org)\"\n",
    "abstract: \"This notebook creates README.md documentation and outputs a file.\"\n",
    "date: now\n",
    "execute:\n",
    "  echo: false\n",
    "  html_document:\n",
    "    fig_width: 8\n",
    "    highlight: haddock\n",
    "    keep_md: false\n",
    "    theme: cerulean\n",
    "    toc: yes\n",
    "    toc_float:\n",
    "      collapsed: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a737a-2729-4624-8df3-c3d01c60e0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import glob\n",
    "import ast\n",
    "import logging\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bac761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    canvas.marks { display: block; margin: auto; }\n",
       "    div.vega-embed { width: 100%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\n",
    "    \"\"\"\n",
    "<style>\n",
    "    canvas.marks { display: block; margin: auto; }\n",
    "    div.vega-embed { width: 100%; }\n",
    "</style>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e162cf4-e94b-483e-937b-08529bf2dbee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"root\")\n",
    "logging.basicConfig(\n",
    "    format=\"\\033[1;36m(def %(funcName)s %(lineno)s): \\033[1;37m %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "TAB = \"\\t\"\n",
    "title = \"{{ cookiecutter.project_name }}\"\n",
    "project_short_description = \"{{ cookiecutter.project_short_description }}\"\n",
    "project_slug = \"{{ cookiecutter.project_slug }}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f917ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_doc_string(script):\n",
    "    \"\"\"\n",
    "    Parses a python file for a docstring and\n",
    "    retrieves the details to output as a dictionary\n",
    "    \"\"\"\n",
    "    # create a container for the output\n",
    "    output = {}\n",
    "    with open(script, \"r\") as file:\n",
    "        # read the file\n",
    "        file_content = file.read()\n",
    "        # parse the module\n",
    "        module = ast.parse(file_content)\n",
    "        # find the docstring in the file\n",
    "        docstring = ast.get_docstring(module)\n",
    "        # index our special characters that set off our make command\n",
    "        start_index = docstring.find(\"_\") + 1\n",
    "        end_index = docstring.find(\"|\")\n",
    "        # let's isolate the make command\n",
    "        if start_index < 0:\n",
    "            output[\"make\"] = None\n",
    "        elif start_index > 0:\n",
    "            output[\"make\"] = docstring[start_index:end_index]\n",
    "        else:\n",
    "            output[\"make\"] = None\n",
    "        # isolate the doc string\n",
    "        output[\"docs\"] = docstring.replace(f\"_{output['make']}|\", \"\").rstrip()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b34b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_js_docstrings():\n",
    "    \"\"\"\n",
    "    Scans a given directory for .js files and extracts text between /** and **/ comment blocks.\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    directory = os.path.join(os.getcwd(), \"scripts\")\n",
    "    markdown_output = []\n",
    "    pattern = re.compile(r\"/\\*\\*(.*?)\\*/\", re.DOTALL)\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".js\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                matches = pattern.findall(content)\n",
    "                if matches:\n",
    "                    for docstring in matches:\n",
    "                        summary = (\n",
    "                            docstring.strip().splitlines()[0].strip()\n",
    "                            if docstring.strip().splitlines()\n",
    "                            else \"No summary available\"\n",
    "                        )\n",
    "                        markdown_output.append(f\"- **{filename}**: {summary}\")\n",
    "\n",
    "    return \"\\n\\n\".join(markdown_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_etl_files():\n",
    "    \"\"\"\n",
    "    Attempt at autodocumentation to list a directory\n",
    "    of etl scripts, pull the name of the file and the doc\n",
    "    string so I can auto document what these things do\n",
    "    \"\"\"\n",
    "    if os.getcwd().split(\"/\")[-1] == \"analysis\":\n",
    "        os.chdir(os.path.dirname(os.getcwd()))\n",
    "    etl_dir = os.path.join(os.getcwd(), \"etl\")\n",
    "    etl_scripts = glob.glob(os.path.join(etl_dir, \"*.py\"))\n",
    "    output = []\n",
    "    sorted_scripts = sorted(etl_scripts)\n",
    "    for script in sorted_scripts:\n",
    "        scriptname = os.path.basename(script)\n",
    "        docstring = return_doc_string(script)\n",
    "        etl_docs_output = \"\"\n",
    "        etl_docs_output += f\"- `{scriptname}`\\n\\n\"\n",
    "        if docstring[\"make\"] != None:\n",
    "            etl_docs_output += f\"{TAB}- {docstring['make']}\\n\\n\"\n",
    "        else:\n",
    "            etl_docs_output += f\"{TAB}- **Make command**: None\\n\\n\"\n",
    "        etl_docs_output += f\"{TAB}- **What it does**: {docstring['docs']}\\n\\n\"\n",
    "        output.append(etl_docs_output)\n",
    "    return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f84f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_docs = document_etl_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_analysis_notebooks():\n",
    "    \"\"\"\n",
    "    Attempt at autodocumentation to list a directory\n",
    "    of analysis notebooks, pull the name of the file\n",
    "    and details from the first cell which contains a\n",
    "    description of what it does.\n",
    "    \"\"\"\n",
    "    # create a path to the analysis notebooks\n",
    "    path = os.path.join(os.getcwd(), \"analysis\")\n",
    "    # find all of the analysis notebooks\n",
    "    analysis_notebooks = glob.glob(os.path.join(path, \"*.ipynb\"))\n",
    "    # create a holding container\n",
    "    output = []\n",
    "    # sort the notebooks alphabetically\n",
    "    notebooks = sorted(analysis_notebooks)\n",
    "    # loop through the notebooks\n",
    "    for notebook in notebooks:\n",
    "        # get the name of the notebook\n",
    "        notebook_name = os.path.basename(notebook)\n",
    "        # load the notebook content\n",
    "        with open(notebook, \"r\", encoding=\"utf-8\") as f:\n",
    "            notebook = json.load(f)\n",
    "        # access the first cell\n",
    "        first_cell = notebook[\"cells\"][0]\n",
    "        # access the abstract\n",
    "        abstract = first_cell[\"source\"][3].replace(\"abstract: \", \"\").replace('\"', \"\")\n",
    "        # structure some output\n",
    "        notebooks_output = \"\"\n",
    "        # here's the filename of the notebook\n",
    "        notebooks_output += f\"- `{notebook_name}`\\n\\n\"\n",
    "        # here's the abstract of the notebook\n",
    "        notebooks_output += f\"{TAB}- **Prompt it answers**: {abstract}\\n\\n\"\n",
    "        # add it to our container\n",
    "        output.append(notebooks_output)\n",
    "    # return the output\n",
    "    return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b85104",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_notebooks = document_analysis_notebooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f7e920-ae0a-44b2-b2b5-6098e08b6731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bring in data from top_level_analysis for AP findings.\n",
    "# %run \"analysis/top_level_analysis.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af07c8d0-30d3-44d1-b01c-4e310868f3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overview = (\n",
    "    f\"# {title}\\n\\n\"\n",
    "    f\"*Current maintainer(s) Christopher L. Keller (<ckeller@ap.org>)*\\n\\n\"\n",
    "    f\"## Project Overview\\n\\n\"\n",
    "    f\"{project_short_description}.\\n\\n\"\n",
    "    f\"## Project notes\\n\\n\"\n",
    "    f\"### Staff involved\\n\\n\"\n",
    "    f\"*Created by:*\\n\\n\"\n",
    "    f\"Christopher L. Keller<br />\\n\\n\"\n",
    "    f\"Data & Graphics Reporter<br />\\n\\n\"\n",
    "    f\"The Associated Press<br />\\n\\n\"\n",
    "    f\"**Email**: ckeller@ap.org<br />\\n\\n\"\n",
    "    f\"**Phone**: (505) 435-2921<br />\\n\\n\"\n",
    "    f\"*Reporter:*\\n\\n\"\n",
    "    f\"- *TK:*\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49a376fa-f66d-4a94-a6d1-53323b9fdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_notes = (\n",
    "    f\"## Data notes\\n\\n\"\n",
    "    f\"### Data sources\\n\\n\"\n",
    "    f\"*TK:*\\n\\n\"\n",
    "    f\"### AP's findings\\n\\n\"\n",
    "    f\"*TK:*\\n\\n\"\n",
    "    f\"### Metadata\\n\\n\"\n",
    "    f\"*TK:*\\n\\n\"\n",
    "    f\"### Known limitations and Caveats\\n\\n\"\n",
    "    f\"*TK:*\\n\\n\"\n",
    "    f\"### Noteworthy Links\\n\\n\"\n",
    "    f\"*TK:*\\n\\n\"\n",
    "    f\"### Notes\\n\\n\"\n",
    "    f\"*TK:*\\n\\n\"\n",
    "    f\"### Sources\\n\\n\"\n",
    "    f\"*TK:*\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e78a47-0bc2-4cbb-9fa9-deca52d85984",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_and_notebooks = (\n",
    "    f\"## Scripts and Notebooks\\n\\n\"\n",
    "    f\"### ETL prep data for analysis\\n\\n\"\n",
    "    f\"These are found in `/etl`. They are:\\n\\n\"\n",
    "    f\"{etl_docs}\"\n",
    "    f\"### Notebooks for analysis\\n\\n\"\n",
    "    f\"These are found in `/analysis`. They are:\\n\\n\"\n",
    "    f\"{analysis_notebooks}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ca6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "makefile_commands = (\n",
    "    f\"### Makefile Commands\\n\\n\"\n",
    "    f\"**datakit tasks**\\n\\n\"\n",
    "    f\"- dkgitlab: runs `datakit gitlab integrate`\\n\\n\"\n",
    "    f\"- dkdata: runs `datakit data init`\\n\\n\"\n",
    "    f\"- dkdata_push: `datakit data push`\\n\\n\"\n",
    "    f\"- dkdata_pull: runs `datakit data pull`\\n\\n\"\n",
    "    f\"- run_notebook: runs `pipenv run jupyter lab --no-browser`\\n\\n\"\n",
    "    f\"- analysis_files: runs `pipenv run quarto render analysis/*.ipynb --to html --execute`\\n\\n\"\n",
    "    f\"- rmd_to_notebook: runs `pipenv run jupytext --set-formats Rmd,ipynb analysis/*.Rmd`\\n\\n\"\n",
    "    f\"- notebook_to_rmd: runs `pipenv run jupytext --set-formats ipynb,Rmd analysis/*.ipynb`\\n\\n\"\n",
    "    f\"- readme: Moves `readme.ipynb` from the root directory to the `analysis` directory and runs the `quarto render` to overwite the README.md file.\\n\\n\"\n",
    "    f\"**package management tasks**\\n\\n\"\n",
    "    f\"- sync: runs `pipenv sync`\\n\\n\"\n",
    "    f\"- install_linting: runs `pipenv install black flake8`\\n\\n\"\n",
    "    f\"- install_etl: runs `pipenv install p_tqdm`\\n\\n\"\n",
    "    f\"- install_analysis: runs `plotnine`\\n\\n\"\n",
    "    f\"- install_web_scrape: runs `pipenv install requests beautifulsoup4`\\n\\n\"\n",
    "    f\"- install_gis: runs `pipenv install geopandas`\\n\\n\"\n",
    "    f\"**data tasks**\\n\\n\"\n",
    "    f'- check_s3: runs `aws s3 ls --recursive \"s3://data.ap.org/projects/2024/{project_slug}/data\" --profile default`\\n\\n'\n",
    "    f\"**data distribution tasks**\\n\\n\"\n",
    "    f\"- copy_distributed_data: runs `cp <PATH_TO_LOCAL_FILE> data/public`\\n\\n\"\n",
    "    f\"- create_distribution: runs `datakit dworld create --slug campus-protest-arrests-big-picture`\\n\\n\"\n",
    "    f\"- upload_distribution: runs `datakit dworld push`\\n\\n\"\n",
    "    f\"- update_documentation: runs `datakit dworld summary`\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405c4af-f7ec-4b5c-81fc-3008a531a2c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "technical = (\n",
    "    f\"## Technical\\n\\n\"\n",
    "    f\"### Assumptions\\n\\n\"\n",
    "    f\"- You've installed [Homebrew](https://brew.sh/).\\n\\n\"\n",
    "    f\"- You've set up your [python environment](https://github.com/associatedpress/cookiecutter-python-project#full-virtual-environment-setup-from-package-management-to-rendering-analyses) with [Pyenv](https://github.com/pyenv/pyenv) to manage our python installations and [Pipenv](https://pipenv.pypa.io/en/latest/) to manage the python packages.\\n\\n\"\n",
    "    f\"- You've setup datakit and configured the various datakit plugins.\\n\\n\"\n",
    "    f\"### Rebuilding or updating the project\\n\\n\"\n",
    "    f\"- Clone the repo down\\n\\n\"\n",
    "    f\"{TAB}- `git clone git@gitlab.inside.ap.org:data/{project_slug}.git`\\n\\n\"\n",
    "    f\"- Change into directory\\n\\n\"\n",
    "    f\"{TAB}- `cd {project_slug}`\\n\\n\"\n",
    "    f\"- Build the datakit config\\n\\n\"\n",
    "    f\"{TAB}- `pipenv run python .first_install.py`\\n\\n\"\n",
    "    f\"- Install the python dependencies from the Pipfile\\n\\n\"\n",
    "    f\"{TAB}- `make sync`\\n\\n\"\n",
    "    f\"- Retrieve the data files\\n\\n\"\n",
    "    f\"{TAB}- `datakit data pull`\\n\\n\"\n",
    "    f\"### Closing out\\n\\n\"\n",
    "    f\"- When finished, push the data files back up\\n\\n\"\n",
    "    f\"{TAB} - `datakit data push` to send the data files up to AWS.\\n\\n\"\n",
    "    f\"- Check to see if the data is stored in the project's S3 bucket\\n\\n\"\n",
    "    f\"{TAB}  - `aws s3 ls --recursive 's3://data.ap.org/projects/2024/{project_slug}/data' --profile default` \\n\\n\"\n",
    "    # f\"### Building the project from scratch\\n\\n\"\n",
    "    # f\"- `datakit project create`\\n\\n\"\n",
    "    # f\"{TAB}- You will be prompted to add:\\n\\n\"\n",
    "    # f\"{TAB}- your first and last name\\n\\n\"\n",
    "    # f\"{TAB}- email\\n\\n\"\n",
    "    # f\"{TAB}- project name\\n\\n\"\n",
    "    # f\"{TAB}- project_slug\\n\\n\"\n",
    "    # f\"{TAB}- project_short_description\\n\\n\"\n",
    "    # f\"- `datakit gitlab integrate`: Code is all tracked in version control and hosted on Gitlab. Important analysis code doesn't live on only one computer, and a detailed revision history is now an assumed feature of all projects. The integration automatically creates a project in gitlab, ready for the reporter to push commits. Issues can be quickly filed using gitlab issues add without having to use the web interface.\\n\\n\"\n",
    "    # f\"- Initialize project to use S3 to store data files\\n\\n\"\n",
    "    # f\"{TAB}- `datakit data init`\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7bc020-c25d-4b11-9519-8f7b213a9ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the holding tank for our README.md file\n",
    "readme = (\n",
    "    f\"{overview}\"\n",
    "    f\"{data_notes}\"\n",
    "    f\"{scripts_and_notebooks}\"\n",
    "    f\"{makefile_commands}\"\n",
    "    f\"{technical}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "300ea873-4738-45e8-bceb-c0445daa7012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file in write mode ('w')\n",
    "with open(\"./README.md\", \"w\") as file:\n",
    "    # Write content to the file\n",
    "    file.write(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d05afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "md(readme)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "campus-protest-arrests-big-picture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
